import requests
from bs4 import BeautifulSoup
import json
import os
import csv # ğŸ’¡ æ–°å¢ï¼šè™•ç† CSV å¿…å‚™
from io import StringIO # ğŸ’¡ æ–°å¢ï¼šè™•ç†æ–‡å­—ä¸²æµ

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_soup(url):
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        res.raise_for_status()
        res.encoding = 'utf-8'
        return BeautifulSoup(res.text, 'html.parser')
    except Exception as e:
        print(f"ç„¡æ³•é€£ç·šè‡³ {url}: {e}")
        return None

# --- 1. è‡ªå‹•å·¡é‚ç¶²ç«™é‚è¼¯ (ä¿æŒåŸæœ‰é€»è¾‘ï¼Œå¢åŠ é˜²å‘†) ---

def scrape_wablieft():
    url = "http://www.wablieft.be/nl/krant"
    soup = get_soup(url)
    results = []
    if soup:
        for item in soup.select('.views-row')[:3]: 
            link_tag = item.select_one('h2 a')
            if link_tag:
                results.append({
                    "title": link_tag.get_text(strip=True),
                    "url": "http://www.wablieft.be" + link_tag['href'],
                    "source": "Wablieft (Easy)",
                    "content": "å°ˆç‚ºå­¸ç¿’è€…è¨­è¨ˆçš„ç°¡åŒ–æ–°èã€‚"
                })
    return results

def scrape_metro():
    url = "https://nl.metrotime.be/onspanning"
    soup = get_soup(url)
    results = []
    if soup:
        # âš ï¸ Metro çš„é¸å–å™¨å¾ˆå¸¸æ›ï¼Œå¦‚æœè·‘ä¸å‹•è«‹æª¢æŸ¥æ­¤è™•
        for item in soup.select('article')[:3]:
            title_tag = item.select_one('h2')
            link_tag = item.select_one('a')
            if title_tag and link_tag:
                results.append({
                    "title": title_tag.get_text(strip=True),
                    "url": link_tag['href'] if link_tag['href'].startswith('http') else "https://nl.metrotime.be" + link_tag['href'],
                    "source": "Metrotime",
                    "content": "ä¾†è‡ª Metro çš„æœ€æ–°ç”Ÿæ´»èˆ‡æ™‚äº‹ã€‚"
                })
    return results

def scrape_zinin():
    url = "https://zininnederlands.be/"
    soup = get_soup(url)
    results = []
    if soup:
        for item in soup.select('.post-title')[:3]:
            link_tag = item.select_one('a')
            if link_tag:
                results.append({
                    "title": link_tag.get_text(strip=True),
                    "url": link_tag['href'],
                    "source": "Zin in Nederlands",
                    "content": "è·è˜­èªå­¸ç¿’æŠ€å·§èˆ‡æ—¥å¸¸ç”¨æ³•ã€‚"
                })
    return results

def scrape_nedbox():
    url = "https://www.nedbox.be/nieuws"
    soup = get_soup(url)
    results = []
    if soup:
        for item in soup.select('.views-row')[:3]:
            title_tag = item.select_one('.field-content a')
            if title_tag:
                results.append({
                    "title": title_tag.get_text(strip=True),
                    "url": "https://www.nedbox.be" + title_tag['href'],
                    "source": "NedBox",
                    "content": "äº’å‹•å¼å­¸ç¿’æ–°èå…§å®¹ã€‚"
                })
    return results

# --- 2. å¦³çš„ Google è©¦ç®—è¡¨æ‰‹å‹•è³‡æ–™åº« (é‡å¤§ä¿®æ­£) ---

def scrape_google_sheet():
    csv_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vSyPZtfBk2ED0-JBkhF0hTstrvp67v6sr5ndwAGQT8miCARIm1Bi5otqE58noyso-5Psewp4H4Q4Ogu/pub?output=csv"
    results = []
    try:
        res = requests.get(csv_url, timeout=10)
        res.encoding = 'utf-8'
        
        # ğŸ’¡ æ”¹ç”¨ csv.reader è®€å–ï¼Œé¿å…é€—è™Ÿå°è‡´è³‡æ–™åˆ‡ç¢
        f = StringIO(res.text)
        reader = csv.reader(f)
        next(reader) # è·³éç¬¬ä¸€åˆ—æ¨™é¡Œ
        
        for row in reader:
            if len(row) >= 4:
                results.append({
                    "title": row[0].strip(),
                    "url": row[1].strip(),
                    "source": row[2].strip() or "Kelsey ç²¾é¸",
                    "content": row[3].strip()
                })
    except Exception as e:
        print(f"Google è©¦ç®—è¡¨è®€å–å¤±æ•—: {e}")
    return results

# --- ç¸½æ•´åˆåŸ·è¡Œ ---

def main():
    print("å˜Ÿä»”å·¡é‚éšŠå‡ºå‹•ï¼")
    final_news = []
    
    # æŠ“å–ä¸¦åˆä½µè³‡æ–™
    final_news.extend(scrape_wablieft())
    final_news.extend(scrape_metro())
    final_news.extend(scrape_zinin())
    final_news.extend(scrape_nedbox())
    final_news.extend(scrape_google_sheet())
    
    # éæ¿¾æ‰ç©ºçš„å…§å®¹
    final_news = [n for n in final_news if n['title']]
    
    os.makedirs('data', exist_ok=True)
    
    with open('data/news.json', 'w', encoding='utf-8') as f:
        json.dump(final_news, f, ensure_ascii=False, indent=4)
    
    print(f"æˆåŠŸæ›´æ–°ï¼ç¾åœ¨å…±æœ‰ {len(final_news)} å‰‡æ–°èè³‡æ–™ã€‚")

if __name__ == "__main__":
    main()
